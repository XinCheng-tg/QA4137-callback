{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0249c028",
   "metadata": {},
   "source": [
    "### Connect to TigerGraph\n",
    "\n",
    "The `TigerGraphConnection` class represents a connection to the TigerGraph database. Under the hood, it stores the necessary information to communicate with the database. It is able to perform quite a few database tasks. Please see its [documentation](https://docs.tigergraph.com/pytigergraph/current/intro/) for details.\n",
    "\n",
    "To connect your database, modify the `config.json` file accompanying this notebook. Set the value of `getToken` based on whether token auth is enabled for your database. Token auth is always enabled for tgcloud databases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2565a60",
   "metadata": {},
   "source": [
    "### Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d892ae1-357d-43c1-bdca-275a5c68e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyTigerGraph import TigerGraphConnection\n",
    "import json\n",
    "\n",
    "# Read in DB configs\n",
    "with open('../../config.json', \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    \n",
    "conn = TigerGraphConnection(\n",
    "    host=config[\"host\"],\n",
    "    username=config[\"username\"],\n",
    "    password=config[\"password\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6e826a2-edff-4dfc-9cdf-063ea61782a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A folder with name Cora already exists in ./tmp. Skip downloading.\n",
      "---- Checking database ----\n",
      "A graph with name Cora already exists in the database. Skip ingestion.\n",
      "Graph name is set to Cora for this connection.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a11e275a91e4c17be95de2d0a256bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CytoscapeWidget(cytoscape_layout={'name': 'circle', 'animate': True, 'padding': 1}, cytoscape_style=[{'selectoâ€¦"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyTigerGraph.datasets import Datasets\n",
    "\n",
    "dataset = Datasets(\"Cora\")\n",
    "\n",
    "conn.ingestDataset(dataset, getToken=config[\"getToken\"])\n",
    "\n",
    "from pyTigerGraph.visualization import drawSchema\n",
    "\n",
    "drawSchema(conn.getSchema(force=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c862e3-bec2-4a71-84aa-fa8f0779292b",
   "metadata": {},
   "source": [
    "# Testcase1: using nodepieceLoader with callback_fn to loaddata.(for homogeneous graph)  \n",
    "## Results: run successfully, data loaded completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9ff0f1e-c474-4275-98fa-7696775ae770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Anchors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyTigerGraph/gds/dataloaders.py\", line 587, in _read_data\n",
      "    data = BaseLoader._parse_data(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyTigerGraph/gds/dataloaders.py\", line 965, in _parse_data\n",
      "    return callback_fn(data)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyTigerGraph/gds/dataloaders.py\", line 3278, in nodepiece_process\n",
      "    ancs = data[\"closest_anchors\"].apply(lambda x: processAnchors(x))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pandas/core/series.py\", line 4774, in apply\n",
      "    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\", line 1100, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\", line 1151, in apply_standard\n",
      "    mapped = lib.map_infer(\n",
      "  File \"pandas/_libs/lib.pyx\", line 2919, in pandas._libs.lib.map_infer\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyTigerGraph/gds/dataloaders.py\", line 3278, in <lambda>\n",
      "    ancs = data[\"closest_anchors\"].apply(lambda x: processAnchors(x))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyTigerGraph/gds/dataloaders.py\", line 3258, in processAnchors\n",
      "    ancs = row.split(\" \")[:-1]\n",
      "AttributeError: 'float' object has no attribute 'split'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 18\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[1;32m      5\u001b[0m np_loader_test01 \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgds\u001b[38;5;241m.\u001b[39mnodepieceLoader(filter_by \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                                      batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m      7\u001b[0m                                      compute_anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m                                      callback_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: process_batch(x)\n\u001b[1;32m     17\u001b[0m                                            )\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(np_loader_test01):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----Batch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m----\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i))\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_key \u001b[38;5;129;01min\u001b[39;00m batch:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyTigerGraph/gds/dataloaders.py:1219\u001b[0m, in \u001b[0;36mBaseLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def process_batch(batch):\n",
    "    return batch\n",
    "\n",
    "np_loader_test01 = conn.gds.nodepieceLoader(filter_by =None,\n",
    "                                     batch_size = 128,\n",
    "                                     compute_anchors = True,\n",
    "                                     clear_cache = True,\n",
    "                                     anchor_percentage = 1,\n",
    "                                     v_feats = [\"y\",\"x\"], \n",
    "                                     target_vertex_types=None, \n",
    "                                     max_anchors=5,\n",
    "                                     max_relational_context=5,\n",
    "                                     e_types=conn.getEdgeTypes(),\n",
    "                                     timeout=204_800_000,\n",
    "                                     callback_fn = lambda x: process_batch(x)\n",
    "                                           )\n",
    "for i, batch in enumerate(np_loader_test01):\n",
    "    print(\"----Batch {}----\".format(i))\n",
    "    for batch_key in batch:\n",
    "        print(\"batch type:\", batch_key)\n",
    "        print(\"batch type dim:\", batch[batch_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb81024",
   "metadata": {},
   "source": [
    "### IMDB dataset\n",
    "We train the model on the IMDB dataset from [PyG datasets](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.IMDB) with TigerGraph as the data store. The dataset contains 3 types of vertices: 4278 movies, 5257 actors, and 2081 directors; and 4 types of edges: 12828 actor to movie edges, 12828 movie to actor edges, 4278 director to movie edges, and 4278 movie to director edges. Each vertex is described by a 0/1-valued word vector indicating the absence/presence of the corresponding keywords from the plot (for movie) or from movies they participated (for actors and directors). Each movie is classified into one of three classes, action, comedy, and drama according to their genre. The goal is to predict the class of each movie in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307a1b22-8853-408b-9c62-788b4376d481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A folder with name imdb already exists in ./tmp. Skip downloading.\n",
      "---- Checking database ----\n",
      "A graph with name imdb already exists in the database. Skip ingestion.\n",
      "Graph name is set to imdb for this connection.\n"
     ]
    }
   ],
   "source": [
    "from pyTigerGraph.datasets import Datasets\n",
    "\n",
    "dataset = Datasets(\"imdb\")\n",
    "\n",
    "conn.ingestDataset(dataset, getToken=config[\"getToken\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2e5e8b-201e-4911-bf3a-9fe657cdb24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9554895",
   "metadata": {},
   "source": [
    "### Visualize Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8971ad89-2740-4ba7-809a-126f6c33066c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdadcdd2db142e699dcbf488cdeb41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CytoscapeWidget(cytoscape_layout={'name': 'circle', 'animate': True, 'padding': 1}, cytoscape_style=[{'selectoâ€¦"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyTigerGraph.visualization import drawSchema\n",
    "\n",
    "drawSchema(conn.getSchema(force=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48896d67-0888-4134-b18b-fe1e006dea8f",
   "metadata": {},
   "source": [
    "# Testcase2: using nodepieceLoader with callback_fn to loaddata.(for heterogeneous graph)    \n",
    "## Results: run successfully, data loaded completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "287a0f94-ebbc-40c9-83b7-72941b667ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Anchors: 1161\n",
      "----Batch 0----\n",
      "batch type: Movie\n",
      "batch type dim:           vid   relational_context  y  \\\n",
      "0     1048608  [12, 12, 12, 13, 0]  0   \n",
      "1     1048644  [12, 12, 12, 13, 0]  2   \n",
      "2     1048680  [12, 12, 12, 13, 0]  1   \n",
      "3     1048688  [12, 12, 12, 13, 0]  1   \n",
      "4     1048696  [12, 12, 12, 13, 0]  2   \n",
      "..        ...                  ... ..   \n",
      "100  19923068  [12, 12, 12, 13, 0]  2   \n",
      "101  25165828  [12, 12, 12, 13, 0]  1   \n",
      "102  27263060  [12, 12, 12, 13, 0]  2   \n",
      "103  27263072  [12, 12, 12, 13, 0]  1   \n",
      "104  27263116  [12, 12, 12, 13, 0]  1   \n",
      "\n",
      "                                                     x  \\\n",
      "0    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "1    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "2    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "3    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "4    0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "..                                                 ...   \n",
      "100  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "101  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "102  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "103  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "104  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "\n",
      "                            anchors anchor_distances  \n",
      "0       [1054, 917, 893, 867, 1117]  [7, 7, 7, 7, 7]  \n",
      "1       [499, 1126, 1126, 860, 548]  [5, 6, 7, 7, 7]  \n",
      "2        [781, 512, 321, 1100, 512]  [5, 5, 5, 6, 6]  \n",
      "3         [398, 247, 398, 398, 399]  [4, 5, 6, 6, 6]  \n",
      "4     [1033, 1033, 886, 1033, 1033]  [7, 9, 9, 9, 9]  \n",
      "..                              ...              ...  \n",
      "100        [65, 120, 571, 478, 444]  [7, 7, 7, 7, 7]  \n",
      "101  [1108, 1108, 1108, 1108, 1108]  [5, 7, 7, 7, 7]  \n",
      "102        [887, 887, 92, 887, 534]  [5, 6, 6, 7, 7]  \n",
      "103       [399, 259, 398, 399, 399]  [4, 5, 6, 6, 6]  \n",
      "104       [740, 315, 740, 740, 315]  [5, 5, 7, 7, 7]  \n",
      "\n",
      "[105 rows x 6 columns]\n",
      "----Batch 1----\n",
      "batch type: Movie\n",
      "batch type dim:           vid   relational_context  y  \\\n",
      "0     4194313  [12, 12, 12, 13, 0]  0   \n",
      "1     4194361  [12, 12, 12, 13, 0]  2   \n",
      "2     4194405  [12, 12, 12, 13, 0]  0   \n",
      "3     4194413  [12, 12, 12, 13, 0]  1   \n",
      "4     4194437  [12, 12, 12, 13, 0]  2   \n",
      "..        ...                  ... ..   \n",
      "105  31457413  [12, 12, 12, 13, 0]  1   \n",
      "106  32505865  [12, 12, 12, 13, 0]  1   \n",
      "107  32505873  [12, 12, 12, 13, 0]  1   \n",
      "108  32505933  [12, 12, 12, 13, 0]  1   \n",
      "109  32505965  [12, 12, 12, 13, 0]  2   \n",
      "\n",
      "                                                     x  \\\n",
      "0    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "1    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "2    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "3    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "4    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "..                                                 ...   \n",
      "105  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "106  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "107  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "108  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "109  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "\n",
      "                        anchors anchor_distances  \n",
      "0    [373, 973, 973, 463, 1122]  [6, 6, 7, 7, 7]  \n",
      "1     [463, 463, 420, 463, 463]  [5, 7, 7, 7, 7]  \n",
      "2     [641, 641, 641, 641, 732]  [5, 7, 7, 7, 7]  \n",
      "3       [983, 189, 1052, 17, 0]  [9, 9, 9, 9, 0]  \n",
      "4     [747, 100, 747, 502, 747]  [7, 8, 9, 9, 9]  \n",
      "..                          ...              ...  \n",
      "105   [226, 308, 493, 519, 392]  [7, 7, 7, 7, 7]  \n",
      "106   [579, 913, 462, 579, 938]  [5, 7, 7, 7, 7]  \n",
      "107  [137, 500, 776, 1167, 703]  [7, 7, 7, 7, 7]  \n",
      "108             [0, 0, 0, 0, 0]  [0, 0, 0, 0, 0]  \n",
      "109        [45, 45, 45, 45, 45]  [5, 7, 7, 7, 7]  \n",
      "\n",
      "[110 rows x 6 columns]\n",
      "----Batch 2----\n",
      "batch type: Movie\n",
      "batch type dim:           vid   relational_context  y  \\\n",
      "0          46  [12, 12, 12, 13, 0]  2   \n",
      "1          86  [12, 12, 12, 13, 0]  1   \n",
      "2          98  [12, 12, 12, 13, 0]  0   \n",
      "3         102  [12, 12, 12, 13, 0]  0   \n",
      "4         110  [12, 12, 12, 13, 0]  2   \n",
      "..        ...                  ... ..   \n",
      "101  15728774  [12, 12, 12, 13, 0]  2   \n",
      "102  25165862  [12, 12, 12, 13, 0]  2   \n",
      "103  25165914  [12, 12, 12, 13, 0]  1   \n",
      "104  25165934  [12, 12, 12, 13, 0]  2   \n",
      "105  25165942  [12, 12, 12, 13, 0]  1   \n",
      "\n",
      "                                                     x  \\\n",
      "0    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "1    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "2    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "3    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "4    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "..                                                 ...   \n",
      "101  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "102  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "103  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "104  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "105  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "\n",
      "                          anchors anchor_distances  \n",
      "0       [405, 111, 435, 609, 722]  [6, 7, 7, 7, 7]  \n",
      "1       [417, 417, 509, 417, 425]  [5, 7, 7, 7, 7]  \n",
      "2        [62, 722, 421, 750, 887]  [7, 7, 7, 7, 7]  \n",
      "3       [260, 260, 260, 260, 260]  [5, 7, 7, 7, 7]  \n",
      "4    [1101, 142, 1101, 1101, 142]  [4, 5, 6, 6, 6]  \n",
      "..                            ...              ...  \n",
      "101     [313, 313, 425, 428, 157]  [5, 7, 7, 7, 7]  \n",
      "102    [276, 1115, 489, 574, 428]  [5, 7, 7, 7, 7]  \n",
      "103     [232, 522, 1074, 21, 854]  [7, 7, 7, 7, 7]  \n",
      "104   [1141, 1141, 887, 463, 969]  [5, 7, 7, 7, 7]  \n",
      "105   [287, 1122, 1122, 287, 287]  [4, 5, 6, 6, 6]  \n",
      "\n",
      "[106 rows x 6 columns]\n",
      "----Batch 3----\n",
      "batch type: Movie\n",
      "batch type dim:          vid   relational_context  y  \\\n",
      "0         15  [12, 12, 12, 13, 0]  1   \n",
      "1         31  [12, 12, 12, 13, 0]  1   \n",
      "2    2097179  [12, 12, 12, 13, 0]  1   \n",
      "3    2097231  [12, 12, 12, 13, 0]  0   \n",
      "4    2097255  [12, 12, 12, 13, 0]  2   \n",
      "..       ...                  ... ..   \n",
      "74  31457331  [12, 12, 12, 13, 0]  0   \n",
      "75  31457359  [12, 12, 12, 13, 0]  0   \n",
      "76  31457375  [12, 12, 12, 13, 0]  2   \n",
      "77  31457383  [12, 12, 12, 13, 0]  2   \n",
      "78  31457407  [12, 12, 12, 13, 0]  2   \n",
      "\n",
      "                                                    x  \\\n",
      "0   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "1   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "2   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "3   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "4   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "..                                                ...   \n",
      "74  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "75  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "76  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "77  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "78  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
      "\n",
      "                           anchors anchor_distances  \n",
      "0        [492, 857, 716, 857, 492]  [5, 5, 7, 7, 7]  \n",
      "1      [1072, 764, 643, 1072, 764]  [4, 5, 5, 6, 6]  \n",
      "2       [189, 189, 170, 189, 1173]  [5, 7, 7, 7, 7]  \n",
      "3      [1167, 1013, 515, 416, 971]  [5, 7, 7, 7, 7]  \n",
      "4       [180, 499, 1126, 499, 499]  [7, 7, 8, 8, 8]  \n",
      "..                             ...              ...  \n",
      "74      [401, 1037, 401, 673, 477]  [6, 7, 7, 7, 7]  \n",
      "75  [1162, 1162, 1162, 1162, 1162]  [5, 7, 7, 7, 7]  \n",
      "76       [425, 425, 425, 798, 425]  [5, 7, 7, 7, 7]  \n",
      "77      [207, 285, 285, 149, 1134]  [5, 7, 7, 7, 7]  \n",
      "78           [38, 38, 99, 376, 38]  [7, 8, 8, 9, 9]  \n",
      "\n",
      "[79 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_loader_test02 = conn.gds.nodepieceLoader(filter_by = \"train_mask\",\n",
    "                                     batch_size = 128,\n",
    "                                     compute_anchors = True,\n",
    "                                     clear_cache = True,\n",
    "                                     anchor_percentage = 0.1,\n",
    "                                     v_feats = {\"Movie\": [\"y\", \"x\"], \"Actor\": [], \"Director\": []}, \n",
    "                                     target_vertex_types=[\"Movie\"], \n",
    "                                     max_anchors=5,\n",
    "                                     max_relational_context=5,\n",
    "                                     e_types=conn.getEdgeTypes(),\n",
    "                                     timeout=204_800_000,\n",
    "                                     )\n",
    "for i, batch in enumerate(np_loader_test02):\n",
    "    print(\"----Batch {}----\".format(i))\n",
    "    for batch_key in batch:\n",
    "        print(\"batch type:\", batch_key)\n",
    "        print(\"batch type dim:\", batch[batch_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8fb2ad1-26e4-4f86-99f6-1188ddedc765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Anchors: 1161\n",
      "----Batch 0----\n",
      "batch type: relational_context\n",
      "batch type dim: torch.Size([105, 5])\n",
      "sample lastone in batch:tensor([12, 12, 12, 13,  0])\n",
      "\n",
      "batch type: anchors\n",
      "batch type dim: torch.Size([105, 5])\n",
      "sample lastone in batch:tensor([742, 695, 695, 695, 742])\n",
      "\n",
      "batch type: distance\n",
      "batch type dim: torch.Size([105, 5])\n",
      "sample lastone in batch:tensor([7, 7, 9, 9, 9])\n",
      "\n",
      "batch type: feats\n",
      "batch type dim: torch.Size([105, 3066])\n",
      "sample lastone in batch:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "batch type: y\n",
      "batch type dim: torch.Size([105])\n",
      "sample lastone in batch:1\n",
      "\n",
      "----Batch 1----\n",
      "batch type: relational_context\n",
      "batch type dim: torch.Size([110, 5])\n",
      "sample lastone in batch:tensor([12, 12, 12, 13,  0])\n",
      "\n",
      "batch type: anchors\n",
      "batch type dim: torch.Size([110, 5])\n",
      "sample lastone in batch:tensor([ 337,   34,  337, 1132,  337])\n",
      "\n",
      "batch type: distance\n",
      "batch type dim: torch.Size([110, 5])\n",
      "sample lastone in batch:tensor([5, 7, 7, 7, 7])\n",
      "\n",
      "batch type: feats\n",
      "batch type dim: torch.Size([110, 3066])\n",
      "sample lastone in batch:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "batch type: y\n",
      "batch type dim: torch.Size([110])\n",
      "sample lastone in batch:2\n",
      "\n",
      "----Batch 2----\n",
      "batch type: relational_context\n",
      "batch type dim: torch.Size([106, 5])\n",
      "sample lastone in batch:tensor([12, 12, 12, 13,  0])\n",
      "\n",
      "batch type: anchors\n",
      "batch type dim: torch.Size([106, 5])\n",
      "sample lastone in batch:tensor([691,  21,  21, 691, 691])\n",
      "\n",
      "batch type: distance\n",
      "batch type dim: torch.Size([106, 5])\n",
      "sample lastone in batch:tensor([4, 5, 6, 6, 6])\n",
      "\n",
      "batch type: feats\n",
      "batch type dim: torch.Size([106, 3066])\n",
      "sample lastone in batch:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "batch type: y\n",
      "batch type dim: torch.Size([106])\n",
      "sample lastone in batch:2\n",
      "\n",
      "----Batch 3----\n",
      "batch type: relational_context\n",
      "batch type dim: torch.Size([79, 5])\n",
      "sample lastone in batch:tensor([12, 12, 12, 13,  0])\n",
      "\n",
      "batch type: anchors\n",
      "batch type dim: torch.Size([79, 5])\n",
      "sample lastone in batch:tensor([1038,  274, 1038, 1038, 1038])\n",
      "\n",
      "batch type: distance\n",
      "batch type dim: torch.Size([79, 5])\n",
      "sample lastone in batch:tensor([5, 7, 7, 7, 7])\n",
      "\n",
      "batch type: feats\n",
      "batch type dim: torch.Size([79, 3066])\n",
      "sample lastone in batch:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "batch type: y\n",
      "batch type dim: torch.Size([79])\n",
      "sample lastone in batch:1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def process_batch(batch):\n",
    "    x = {\"relational_context\": torch.tensor(batch[\"Movie\"][\"relational_context\"], dtype=torch.long), \n",
    "         \"anchors\": torch.tensor(batch[\"Movie\"][\"anchors\"], dtype=torch.long), \n",
    "         \"distance\": torch.tensor(batch[\"Movie\"][\"anchor_distances\"], dtype=torch.long),\n",
    "         \"feats\": torch.tensor(np.stack(batch[\"Movie\"][\"x\"].apply(lambda x: np.fromstring(x, sep=\" \")).values), dtype=torch.float),\n",
    "         \"y\": torch.tensor(batch[\"Movie\"][\"y\"].astype(int))}\n",
    "    return x\n",
    "\n",
    "np_loader_test03 = conn.gds.nodepieceLoader(filter_by = \"train_mask\",\n",
    "                                     batch_size = 128,\n",
    "                                     compute_anchors = True,\n",
    "                                     clear_cache = True,\n",
    "                                     anchor_percentage = 0.1,\n",
    "                                     v_feats = {\"Movie\": [\"y\", \"x\"], \"Actor\": [], \"Director\": []}, \n",
    "                                     target_vertex_types=[\"Movie\"], \n",
    "                                     max_anchors=5,\n",
    "                                     max_relational_context=5,\n",
    "                                     e_types=conn.getEdgeTypes(),\n",
    "                                     timeout=204_800_000,\n",
    "                                     callback_fn = lambda x: process_batch(x))\n",
    "for i, batch in enumerate(np_loader_test03):\n",
    "    print(\"----Batch {}----\".format(i))\n",
    "    for batch_key in batch:\n",
    "        print(\"batch type:\", batch_key)\n",
    "        print(\"batch type dim:\", batch[batch_key].size())\n",
    "        print(\"sample lastone in batch:{}\\n\".format(batch[batch_key][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293450f5-8839-4835-af6e-6a029188f607",
   "metadata": {},
   "source": [
    "# Testcase3: using callback_fn in nodepieceLoader to create both train and valid data, then train a model.  \n",
    "## Results:run successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ee850",
   "metadata": {},
   "source": [
    "## NodePiece Algorithm <a name=\"nodepiece_algorithm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28632ef5",
   "metadata": {},
   "source": [
    "The [NodePiece algorithm](https://arxiv.org/abs/2106.12144) was introduced as a way to both conserve the memory cost of vertex embeddings, as well as be able to generalize to unseen vertices during the testing process. This makes NodePiece a much more scalable approach for large, real-world graphs compared to other transductive techniques such as FastRP or Node2Vec. For more information about the algorithm, check out the author's [Medium post](https://towardsdatascience.com/nodepiece-tokenizing-knowledge-graphs-6dd2b91847aa).\n",
    "\n",
    "We implement the NodePiece dataloader, which will allow us to iterate through batches of vertices. We take advantage of the callback functionality to process the batch into PyTorch tensors for less data manipulation in the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61fb38f",
   "metadata": {},
   "source": [
    "## Train on Vertex Samples <a name=\"train_vertex\"></a>\n",
    "We train the model on batches of vertices. We utilize both the trainable embeddings provided by NodePiece, as well as the `x` feature vector stored as an attribute on all Movie vertices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a955d66",
   "metadata": {},
   "source": [
    "### Construct model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b77deb55-34ca-4dd8-bd85-868f8d4bb67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNodePiece(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size:int,\n",
    "                 sequence_length:int,\n",
    "                 embedding_dim:int=768):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        anc_emb = self.embedding(x[\"anchors\"])\n",
    "        rel_emb = self.embedding(x[\"relational_context\"])\n",
    "        anc_emb += self.embedding(x[\"distance\"])\n",
    "        out = torch.concat([anc_emb, rel_emb], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76558ce2-9030-48c4-82dd-d2f44044ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_model:BaseNodePiece,\n",
    "                 out_dim:int=2,\n",
    "                 num_hidden_layers:int=2,\n",
    "                 hidden_dim:int=128):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_hidden_layers + 2\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers)])\n",
    "        self.out_layer = nn.Linear(hidden_dim, out_dim)\n",
    "        self.in_layer = nn.Linear((embedding_model.embedding_dim*embedding_model.sequence_length)+3066, hidden_dim)\n",
    "        self.emb_model = embedding_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = x[\"feats\"]\n",
    "        x = self.emb_model(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = torch.cat((x, feats), dim=1)\n",
    "        x = self.in_layer(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.dropout(F.relu(layer(x)), p=0.6)\n",
    "        x = self.out_layer(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a85b71-a507-4498-b76e-16def52accb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    x = {\"relational_context\": torch.tensor(batch[\"Movie\"][\"relational_context\"], dtype=torch.long), \n",
    "         \"anchors\": torch.tensor(batch[\"Movie\"][\"anchors\"], dtype=torch.long), \n",
    "         \"distance\": torch.tensor(batch[\"Movie\"][\"anchor_distances\"], dtype=torch.long),\n",
    "         \"feats\": torch.tensor(np.stack(batch[\"Movie\"][\"x\"].apply(lambda x: np.fromstring(x, sep=\" \")).values), dtype=torch.float),\n",
    "         \"y\": torch.tensor(batch[\"Movie\"][\"y\"].astype(int))}\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85cb4bc2-9123-40c2-aa3b-788ef31175a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Anchors: 1161\n"
     ]
    }
   ],
   "source": [
    "np_loader = conn.gds.nodepieceLoader(filter_by = \"train_mask\",\n",
    "                                     batch_size = 128,\n",
    "                                     compute_anchors = True,\n",
    "                                     clear_cache = True,\n",
    "                                     anchor_percentage = 0.1,\n",
    "                                     v_feats = {\"Movie\": [\"y\", \"x\"], \"Actor\": [], \"Director\": []}, \n",
    "                                     target_vertex_types=[\"Movie\"], \n",
    "                                     max_anchors=5,\n",
    "                                     max_relational_context=5,\n",
    "                                     e_types=conn.getEdgeTypes(),\n",
    "                                     timeout=204_800_000,\n",
    "                                     callback_fn = lambda x: process_batch(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab2a4af9-1782-4717-b074-eecb1cf9d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = BaseNodePiece(vocab_size=np_loader.num_tokens, # add in special tokens\n",
    "                 sequence_length=np_loader._payload[\"max_rel_context\"] + np_loader._payload[\"max_anchors\"],\n",
    "                 embedding_dim=128)\n",
    "\n",
    "model = MLP(emb_model, out_dim=3, num_hidden_layers=2, hidden_dim=128)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eb3eb26-4666-43f3-816a-0c1b3cb0c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_loader.saveTokens(\"./npAncs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb23e6cb-5aff-4087-a40e-b5780553c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = conn.gds.nodepieceLoader(anchor_cache_attr=\"anchors\", \n",
    "                                        filter_by = \"val_mask\",\n",
    "                                        batch_size = 8192,\n",
    "                                        v_feats = {\"Movie\": [\"y\", \"x\"], \"Actor\": [], \"Director\": []}, \n",
    "                                        target_vertex_types=[\"Movie\"], \n",
    "                                        compute_anchors=False,\n",
    "                                        max_anchors=5,\n",
    "                                        max_relational_context=5,\n",
    "                                        use_cache = True,\n",
    "                                        e_types=conn.getEdgeTypes(),\n",
    "                                        timeout=204_800_000,\n",
    "                                        tokenMap=\"./npAncs.pkl\",\n",
    "                                        callback_fn = lambda x: process_batch(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5347a5b8",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9618494-ab14-43b1-872b-eec8e6466e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0: 1.095803141593933 Train Accuracy: 0.3975 Time: 0.9867422580718994 Valid Loss: 1.06856107711792 Valid Accuracy: 0.3775\n",
      "EPOCH 1: 1.077799916267395 Train Accuracy: 0.375 Time: 1.099294900894165 Valid Loss: 1.0795835256576538 Valid Accuracy: 0.4125\n",
      "EPOCH 2: 1.021700456738472 Train Accuracy: 0.4925 Time: 1.1532020568847656 Valid Loss: 1.0630719661712646 Valid Accuracy: 0.4575\n",
      "EPOCH 3: 0.7498434334993362 Train Accuracy: 0.7275 Time: 1.29939866065979 Valid Loss: 1.2397489547729492 Valid Accuracy: 0.475\n",
      "EPOCH 4: 0.3719747066497803 Train Accuracy: 0.9 Time: 1.25838303565979 Valid Loss: 2.004598379135132 Valid Accuracy: 0.45\n",
      "EPOCH 5: 0.11212521605193615 Train Accuracy: 0.9725 Time: 1.348665475845337 Valid Loss: 3.170536756515503 Valid Accuracy: 0.4625\n",
      "EPOCH 6: 0.06731609907001257 Train Accuracy: 0.98 Time: 1.4826509952545166 Valid Loss: 3.757309675216675 Valid Accuracy: 0.4625\n",
      "EPOCH 7: 0.030457580054644495 Train Accuracy: 0.9925 Time: 1.5900390148162842 Valid Loss: 4.077134609222412 Valid Accuracy: 0.4975\n",
      "EPOCH 8: 0.01071771104761865 Train Accuracy: 0.9975 Time: 1.570281744003296 Valid Loss: 5.813042163848877 Valid Accuracy: 0.5025\n",
      "EPOCH 9: 0.09219846036285162 Train Accuracy: 0.99 Time: 1.6613671779632568 Valid Loss: 4.561295032501221 Valid Accuracy: 0.4925\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from pyTigerGraph.gds.metrics import Accuracy\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    acc = Accuracy()\n",
    "    epoch_loss = 0\n",
    "    start = time.time()\n",
    "    for batch in np_loader:\n",
    "        labels = batch[\"y\"]\n",
    "        out = model(batch)\n",
    "        loss_val = loss(out, labels)\n",
    "        acc.update(out.argmax(dim=1), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_val.item()\n",
    "    end = time.time()\n",
    "    val_acc = Accuracy()\n",
    "    val_epoch_loss = 0\n",
    "    for val_batch in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            labels = val_batch[\"y\"]\n",
    "            out = model(val_batch)\n",
    "            loss_val = loss(out, labels)\n",
    "            val_acc.update(out.argmax(dim=1), labels)\n",
    "            val_epoch_loss += loss_val.item()\n",
    "    print(\"EPOCH {}: {}\".format(i, epoch_loss/np_loader.num_batches), \n",
    "          \"Train Accuracy:\", acc.value, \n",
    "          \"Time:\", end-start, \n",
    "          \"Valid Loss: {}\".format(val_epoch_loss/valid_loader.num_batches), \n",
    "          \"Valid Accuracy:\", val_acc.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b339007-d656-4e94-bc3a-59c7769b9f7b",
   "metadata": {},
   "source": [
    "# Testcase4: using nodepieceLoader with callback_fn to loaddata(via Kafka).  \n",
    "## Results: run successfully, data loaded completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a7e529a-7e46-4ae0-9bea-821b88aa07ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Anchors: 1161\n",
      "----Batch 0----\n",
      "batch type: relational_context\n",
      "batch type dim: torch.Size([105, 5])\n",
      "sample lastone in batch:tensor([12, 12, 12, 13,  0])\n",
      "\n",
      "batch type: anchors\n",
      "batch type dim: torch.Size([105, 5])\n",
      "sample lastone in batch:tensor([466, 144, 144, 144, 466])\n",
      "\n",
      "batch type: distance\n",
      "batch type dim: torch.Size([105, 5])\n",
      "sample lastone in batch:tensor([7, 7, 9, 9, 9])\n",
      "\n",
      "batch type: feats\n",
      "batch type dim: torch.Size([105, 3066])\n",
      "sample lastone in batch:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "batch type: y\n",
      "batch type dim: torch.Size([105])\n",
      "sample lastone in batch:1\n",
      "\n",
      "----Batch 1----\n",
      "batch type: relational_context\n",
      "batch type dim: torch.Size([110, 5])\n",
      "sample lastone in batch:tensor([12, 12, 12, 13,  0])\n",
      "\n",
      "batch type: anchors\n",
      "batch type dim: torch.Size([110, 5])\n",
      "sample lastone in batch:tensor([ 492,  815, 1101,  181,  150])\n",
      "\n",
      "batch type: distance\n",
      "batch type dim: torch.Size([110, 5])\n",
      "sample lastone in batch:tensor([7, 7, 7, 7, 7])\n",
      "\n",
      "batch type: feats\n",
      "batch type dim: torch.Size([110, 3066])\n",
      "sample lastone in batch:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "batch type: y\n",
      "batch type dim: torch.Size([110])\n",
      "sample lastone in batch:1\n",
      "\n",
      "----Batch 2----\n",
      "batch type: relational_context\n",
      "batch type dim: torch.Size([106, 5])\n",
      "sample lastone in batch:tensor([12, 12, 12, 13,  0])\n",
      "\n",
      "batch type: anchors\n",
      "batch type dim: torch.Size([106, 5])\n",
      "sample lastone in batch:tensor([ 344, 1142, 1142,  344,  344])\n",
      "\n",
      "batch type: distance\n",
      "batch type dim: torch.Size([106, 5])\n",
      "sample lastone in batch:tensor([4, 5, 6, 6, 6])\n",
      "\n",
      "batch type: feats\n",
      "batch type dim: torch.Size([106, 3066])\n",
      "sample lastone in batch:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "batch type: y\n",
      "batch type dim: torch.Size([106])\n",
      "sample lastone in batch:1\n",
      "\n",
      "----Batch 3----\n",
      "batch type: relational_context\n",
      "batch type dim: torch.Size([79, 5])\n",
      "sample lastone in batch:tensor([12, 12, 12, 13,  0])\n",
      "\n",
      "batch type: anchors\n",
      "batch type dim: torch.Size([79, 5])\n",
      "sample lastone in batch:tensor([1062,  143,  860,  276,  990])\n",
      "\n",
      "batch type: distance\n",
      "batch type dim: torch.Size([79, 5])\n",
      "sample lastone in batch:tensor([7, 7, 7, 7, 7])\n",
      "\n",
      "batch type: feats\n",
      "batch type dim: torch.Size([79, 3066])\n",
      "sample lastone in batch:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "batch type: y\n",
      "batch type dim: torch.Size([79])\n",
      "sample lastone in batch:2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def process_batch(batch):\n",
    "    x = {\"relational_context\": torch.tensor(batch[\"Movie\"][\"relational_context\"], dtype=torch.long), \n",
    "         \"anchors\": torch.tensor(batch[\"Movie\"][\"anchors\"], dtype=torch.long), \n",
    "         \"distance\": torch.tensor(batch[\"Movie\"][\"anchor_distances\"], dtype=torch.long),\n",
    "         \"feats\": torch.tensor(np.stack(batch[\"Movie\"][\"x\"].apply(lambda x: np.fromstring(x, sep=\" \")).values), dtype=torch.float),\n",
    "         \"y\": torch.tensor(batch[\"Movie\"][\"y\"].astype(int))}\n",
    "    return x\n",
    "\n",
    "conn.gds.configureKafka(kafka_address =\"kaf.ml.tgcloud.io:19092\")\n",
    "np_loader_test05 = conn.gds.nodepieceLoader(filter_by = \"train_mask\",\n",
    "                                     batch_size = 128,\n",
    "                                     compute_anchors = True,\n",
    "                                     clear_cache = True,\n",
    "                                     anchor_percentage = 0.1,\n",
    "                                     v_feats = {\"Movie\": [\"y\", \"x\"], \"Actor\": [], \"Director\": []}, \n",
    "                                     target_vertex_types=[\"Movie\"], \n",
    "                                     max_anchors=5,\n",
    "                                     max_relational_context=5,\n",
    "                                     e_types=conn.getEdgeTypes(),\n",
    "                                     timeout=204_800_000,\n",
    "                                     callback_fn = lambda x: process_batch(x))\n",
    "for i, batch in enumerate(np_loader_test05):\n",
    "    print(\"----Batch {}----\".format(i))\n",
    "    for batch_key in batch:\n",
    "        print(\"batch type:\", batch_key)\n",
    "        print(\"batch type dim:\", batch[batch_key].size())\n",
    "        print(\"sample lastone in batch:{}\\n\".format(batch[batch_key][-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
